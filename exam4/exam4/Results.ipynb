{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# I Load the  file\n",
    "with np.load('cifar4-train.npz') as data:\n",
    "    cifar4_data = dict(data.items())\n",
    "\n",
    "X=cifar4_data['overfeat']\n",
    "y=cifar4_data['labels']\n",
    "P=cifar4_data['pixels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erobbian\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "\n",
    "# Split data into train/test sets\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Batch generator\n",
    "def get_batches(X, y, batch_size):\n",
    "    # Shuffle X,y\n",
    "    shuffled_idx = np.arange(len(y))\n",
    "    np.random.shuffle(shuffled_idx)\n",
    "    \n",
    "    \n",
    "    for i in range(0, len(y), batch_size):\n",
    "        # Batch indexes\n",
    "        batch_idx = shuffled_idx[i:i+batch_size]\n",
    "        yield X[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[]\n",
    "accuracy_result=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN Accuracy (Test set): 77.5 %\n"
     ]
    }
   ],
   "source": [
    "pipeKNN = Pipeline([\n",
    "    ('PCA', PCA(n_components=87)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=41, p=1))\n",
    "])\n",
    "\n",
    "# Fit to train data\n",
    "pipeKNN.fit(X_tr, y_tr)\n",
    "\n",
    "# Evaluate on test set\n",
    "accuracy = pipeKNN.score(X_te, y_te)*100\n",
    "print('k-NN Accuracy (Test set): {:.1f} %'.format(accuracy))\n",
    "accuracy_result.append(accuracy)\n",
    "model='KNN'\n",
    "models.append(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy (Test set): 65.4 %\n"
     ]
    }
   ],
   "source": [
    "pipeTREE = Pipeline([\n",
    "   \n",
    "    ( 'PCA', PCA(n_components=87)),\n",
    "        ('Dt', DecisionTreeClassifier(max_depth=5))])\n",
    "    \n",
    "    \n",
    "# Fit the tuned decision tree\n",
    "pipeTREE.fit(X_tr, y_tr)\n",
    "\n",
    "#  result\n",
    "\n",
    "accuracy = pipeTREE.score(X_te, y_te)*100\n",
    "print('Decision Tree Accuracy (Test set): {:.1f} %'.format(accuracy))\n",
    "accuracy_result.append(accuracy)\n",
    "model='Decision Tree'\n",
    "models.append(model)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy (Test set): 76.1 %\n"
     ]
    }
   ],
   "source": [
    "pipeforest = Pipeline([\n",
    "    ('PCA', PCA(n_components=87)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=110))\n",
    "])\n",
    "\n",
    "# Fit to train data\n",
    "pipeforest.fit(X_tr, y_tr)\n",
    "\n",
    "accuracy = pipeforest.score(X_te, y_te)*100\n",
    "print('Random Forest Accuracy (Test set): {:.1f} %'.format(accuracy))\n",
    "accuracy_result.append(accuracy)\n",
    "model='Random Forest'\n",
    "models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Linear Kernel Accuracy (Test set): 80.6 %\n"
     ]
    }
   ],
   "source": [
    "pipeLSVM = Pipeline([\n",
    "     ('PCA', PCA(n_components=87)),\n",
    "        ('LSV', LinearSVC(random_state=0, C=0.05)\n",
    "        )])\n",
    "\n",
    "\n",
    "# Fit to train data\n",
    "pipeLSVM.fit(X_tr, y_tr)\n",
    "\n",
    "accuracy = pipeLSVM.score(X_te, y_te)*100\n",
    "print('SVM Linear Kernel Accuracy (Test set): {:.1f} %'.format(accuracy))\n",
    "accuracy_result.append(accuracy)\n",
    "model='SVM Linear Kernel'\n",
    "models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBF Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM RBF Kernel Accuracy (Test set): 81.4 %\n"
     ]
    }
   ],
   "source": [
    "# I ran the tuned Non-linear Kernel\n",
    "pipeSVC = Pipeline([\n",
    "     ('PCA', PCA(n_components=87)),\n",
    "        ('SVC', SVC(C=1.55,gamma=0.0003)\n",
    "        )])\n",
    "\n",
    "# Fit to train data\n",
    "pipeSVC.fit(X_tr, y_tr)\n",
    "\n",
    "accuracy = pipeSVC.score(X_te, y_te)*100\n",
    "print('SVM RBF Kernel Accuracy (Test set): {:.1f} %'.format(accuracy))\n",
    "accuracy_result.append(accuracy)\n",
    "model='SVM RBF Kernel'\n",
    "models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy (Test set): 82.8 %\n"
     ]
    }
   ],
   "source": [
    "pipelogreg = Pipeline([\n",
    "     ('PCA', PCA(n_components=None)),\n",
    "        ('logreg', LogisticRegression(C=0.1)\n",
    "        )])\n",
    "\n",
    "\n",
    "# Fit to train data\n",
    "pipelogreg.fit(X_tr, y_tr)\n",
    "\n",
    "accuracy = pipelogreg.score(X_te, y_te)*100\n",
    "print('Logistic Regression Accuracy (Test set): {:.1f} %'.format(accuracy))\n",
    "accuracy_result.append(accuracy)\n",
    "model='Logistic Regression'\n",
    "models.append(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilayer fully-connected network accuracy (test set) 81.9 %\n"
     ]
    }
   ],
   "source": [
    "# I create the netwrok Graph\n",
    "\n",
    "# Create new graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Create placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 4096])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    \n",
    "    # Hidden layer with 64 units and RELU activation\n",
    "    hidden = tf.layers.dense(\n",
    "        X, 64, activation=tf.nn.relu, # ReLU\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name='hidden'\n",
    "    )\n",
    "    \n",
    "    \n",
    "   \n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        hidden, 10, activation=None, # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=0),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name='output'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Get weights of the first hidden layer\n",
    "    with tf.variable_scope('hidden', reuse=True):\n",
    "        W1 = tf.get_variable('kernel')\n",
    "    \n",
    "    # Dropout\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    flat_output = tf.layers.dropout(hidden, rate=0.5, seed=0, training=training)\n",
    "    \n",
    "        \n",
    "    \n",
    "    #Get weights of the second tensor\n",
    "    with tf.variable_scope('output', reuse=True):\n",
    "        W2 = tf.get_variable('kernel')\n",
    "    \n",
    "    \n",
    "    # Regularization strength\n",
    "    alpha = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "    \n",
    "    # Loss fuction: mean cross-entropy with regularization on W1 and W2\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    regularizer_W1=tf.nn.l2_loss(W1)\n",
    "    regularizer_W2=tf.nn.l2_loss(W2)\n",
    "    \n",
    "    loss=tf.reduce_mean(loss+alpha*regularizer_W1+alpha*regularizer_W2)\n",
    "    \n",
    "    \n",
    "    # Gradient descent\n",
    "    lr = tf.placeholder(dtype=tf.float32)\n",
    "    gd = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "\n",
    "    # Minimize cross-entropy\n",
    "    train_op = gd.minimize(loss)\n",
    "    \n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "    \n",
    "    \n",
    "\n",
    "# I initialize, train and test the results\n",
    "\n",
    "# Validation accuracy\n",
    "valid_acc_values = []\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Set seed\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # Train several epochs\n",
    "    for epoch in range(50):\n",
    "        # Accuracy values (train) after each batch\n",
    "        batch_acc = []\n",
    "        \n",
    "        # Get batches of data\n",
    "        for X_batch, y_batch in get_batches(X_tr, y_tr, 50):\n",
    "            # Run training and evaluate accuracy\n",
    "            _, acc_value = sess.run([train_op, accuracy], feed_dict={\n",
    "                X: X_batch,\n",
    "                y: y_batch,\n",
    "                lr: 0.005, # Learning rate\n",
    "                training: True, # Apply dropout\n",
    "                alpha: 0.01 # Regularization strength\n",
    "\n",
    "            })\n",
    "            \n",
    "            \n",
    "    # Evaluate test accuracy\n",
    "    test_acc = sess.run(accuracy, feed_dict={\n",
    "        X: X_te,\n",
    "        y: y_te\n",
    "\n",
    "    })\n",
    "        \n",
    "    print('Multilayer fully-connected network accuracy (test set) {:.1f} %'.format(test_acc*100))\n",
    "    \n",
    "accuracy_result.append(test_acc*100)\n",
    "model='Multilayer fully-connected network'\n",
    "models.append(model)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erobbian\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Neural Network Test accuracy: 65.8\n"
     ]
    }
   ],
   "source": [
    "# I re-arrange the data\n",
    "\n",
    "P=(P - 128) / 255\n",
    "\n",
    "# and I split them in train and test\n",
    "\n",
    "# Create the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    # Reshape images: 32 by 32 with 3 (RGB) color channels\n",
    "    P.reshape(-1, 32, 32, 3),\n",
    "    cifar4_data['labels'],\n",
    "    test_size=1800, random_state=0)\n",
    "\n",
    "# create the validate set from the train\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_test, y_test, test_size=1000, random_state=0)\n",
    "\n",
    "# Batch generator\n",
    "def get_batches_conv(X, y, batch_size):\n",
    "    # Shuffle X,y\n",
    "    shuffled_idx = np.arange(len(y)) # 1,2,...,n\n",
    "    np.random.shuffle(shuffled_idx)\n",
    "    \n",
    "    # Enumerate indexes by steps of batch_size\n",
    "    # i: 0, b, 2b, 3b, 4b, .. where b is the batch size\n",
    "    for i in range(0, len(y), batch_size):\n",
    "        # Batch indexes\n",
    "        batch_idx = shuffled_idx[i:i+batch_size]\n",
    "        yield X[batch_idx], y[batch_idx]\n",
    "\n",
    "# Create new graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    \n",
    "    \n",
    "    # Convolutional layer (64 filters, 5x5, stride: 2)\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        X, 64, (5, 5), (2, 2), 'SAME', # \"same\" padding\n",
    "        activation=tf.nn.relu, # ReLU\n",
    "        kernel_initializer=tf.truncated_normal_initializer(stddev=0.01, seed=0),\n",
    "        name='conv1'\n",
    "    )\n",
    "        \n",
    "    # Maxpool layer (2x2, stride: 2, \"same\" padding)\n",
    "    pool1 = tf.layers.max_pooling2d(conv1, (2, 2), (2, 2), 'SAME')\n",
    "    \n",
    "    \n",
    "    # Convolutional layer (64 filters, 3x3, stride: 1)\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        pool1, 64, (3, 3), (1, 1), 'SAME', # \"same\" padding\n",
    "        activation=tf.nn.relu, # ReLU\n",
    "        kernel_initializer=tf.truncated_normal_initializer(stddev=0.01, seed=0),\n",
    "        name='conv2'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Maxpool layer (2x2, stride: 2, \"same\" padding)\n",
    "    pool2 = tf.layers.max_pooling2d(conv2, (2, 2), (2, 2), 'SAME')\n",
    "    \n",
    "    \n",
    "    # Flatten output\n",
    "    flat_output = tf.contrib.layers.flatten(pool2)\n",
    "    \n",
    "    \n",
    "    # Dropout\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    flat_output = tf.layers.dropout(flat_output, rate=0.5, seed=0, training=training)\n",
    "    \n",
    "    \n",
    "    # Fully connected layer\n",
    "    fc1 = tf.layers.dense(\n",
    "        flat_output, 256, # 256 hidden units\n",
    "        activation=tf.nn.relu, # ReLU\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "        bias_initializer=tf.zeros_initializer()\n",
    "    )\n",
    "   \n",
    "    \n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        fc1, 4, # One output unit per category\n",
    "        activation=None, # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=0),\n",
    "        bias_initializer=tf.zeros_initializer()\n",
    "    )\n",
    "   \n",
    "    \n",
    "    # Kernel of the 1st conv. layer\n",
    "    with tf.variable_scope('conv1', reuse=True):\n",
    "        conv_kernels = tf.get_variable('kernel')\n",
    "    \n",
    "    # Mean cross-entropy\n",
    "    mean_ce = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=y, logits=logits))\n",
    "    \n",
    "    # Adam optimizer\n",
    "    lr = tf.placeholder(dtype=tf.float32)\n",
    "    gd = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "    # Minimize cross-entropy\n",
    "    train_op = gd.minimize(mean_ce)\n",
    "    \n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "    \n",
    "    \n",
    "# Validation accuracy\n",
    "valid_acc_values = []\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Set seed\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # Train several epochs\n",
    "    for epoch in range(50):\n",
    "        # Accuracy values (train) after each batch\n",
    "        batch_acc = []\n",
    "        \n",
    "        for X_batch, y_batch in get_batches_conv(X_train, y_train, 20):\n",
    "            # Run training and evaluate accuracy\n",
    "            _, acc_value = sess.run([train_op, accuracy], feed_dict={\n",
    "                X: X_batch,\n",
    "                y: y_batch,\n",
    "                lr: 0.0001, # Learning rate\n",
    "                training:True\n",
    "            })\n",
    "            \n",
    "            # Save accuracy (current batch)\n",
    "            batch_acc.append(acc_value)\n",
    "\n",
    "        # Evaluate validation accuracy\n",
    "        valid_acc = sess.run(accuracy, feed_dict={\n",
    "            X: X_valid,\n",
    "            y: y_valid,\n",
    "            training: False\n",
    "        })\n",
    "        valid_acc_values.append(valid_acc)\n",
    "        \n",
    "      \n",
    "        \n",
    "    # Get 1st conv. layer kernels\n",
    "    kernels = conv_kernels.eval()\n",
    "    \n",
    "    # Evaluate test accuracy\n",
    "    test_acc = sess.run(accuracy, feed_dict={\n",
    "        X: X_test,\n",
    "        y: y_test,\n",
    "            training: False\n",
    "    })\n",
    "print('Convolutional Neural Network Test accuracy: {:.1f}'.format(test_acc*100))\n",
    "    \n",
    "accuracy_result.append(test_acc*100)\n",
    "model='Multilayer fully-connected network'\n",
    "models.append(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>82.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Multilayer fully-connected network</td>\n",
       "      <td>81.900001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM RBF Kernel</td>\n",
       "      <td>81.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM Linear Kernel</td>\n",
       "      <td>80.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>77.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>76.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Multilayer fully-connected network</td>\n",
       "      <td>65.799999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>65.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Classifier  Test_Accuracy\n",
       "5                 Logistic Regression      82.800000\n",
       "6  Multilayer fully-connected network      81.900001\n",
       "4                      SVM RBF Kernel      81.400000\n",
       "3                   SVM Linear Kernel      80.600000\n",
       "0                                 KNN      77.500000\n",
       "2                       Random Forest      76.100000\n",
       "7  Multilayer fully-connected network      65.799999\n",
       "1                       Decision Tree      65.400000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Overview=pd.DataFrame(models, columns=['Classifier'])\n",
    "test_accuracy=pd.DataFrame(accuracy_result, columns=['Test_Accuracy'])\n",
    "Overview=Overview.join(test_accuracy, )\n",
    "Overview.sort_values(by='Test_Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considering both accuracy and running time I decide use for the classification the SVC RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I Load the  file for the prediction\n",
    "with np.load('cifar4-test.npz') as data:\n",
    "    cifar4_data = dict(data.items())\n",
    "\n",
    "X_test=cifar4_data['overfeat']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions=pipeSVC.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count\n",
       "Class       \n",
       "0        249\n",
       "1        262\n",
       "2        229\n",
       "3        260"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I check that the prediction is evenly distributed among classes\n",
    "df_predict=pd.DataFrame(test_predictions,columns=['Class'])\n",
    "df_predict['count']=1\n",
    "df_predict.groupby(by='Class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import TemporaryFile\n",
    "export = TemporaryFile()\n",
    "\n",
    "np.save('test_predictions.npy',test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
