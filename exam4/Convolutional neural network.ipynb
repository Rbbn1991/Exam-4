{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a dictionary with keys: ['pixels', 'overfeat', 'labels', 'names', 'allow_pickle']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the .npz file\n",
    "with np.load('cifar4-train.npz') as data:\n",
    "    cifar4_data = dict(data.items())\n",
    "\n",
    "print('It is a dictionary with keys:', list(cifar4_data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=cifar4_data['labels']\n",
    "\n",
    "# I rescale the data\n",
    "P=cifar4_data['pixels']\n",
    "P=(P - 128) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (3200, 32, 32, 3) (3200,)\n",
      "Valid: (800, 32, 32, 3) (800,)\n",
      "Test: (1000, 32, 32, 3) (1000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "# Create train set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    # Reshape images: 32 by 32 with 3 (RGB) color channels\n",
    "    P.reshape(-1, 32, 32, 3),\n",
    "    cifar4_data['labels'],\n",
    "    test_size=1800, random_state=0)\n",
    "\n",
    "# Create validation and test sets\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_test, y_test, test_size=1000, random_state=0)\n",
    "\n",
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Valid:', X_valid.shape, y_valid.shape)\n",
    "print('Test:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch generator\n",
    "def get_batches(X, y, batch_size):\n",
    "    # Shuffle X,y\n",
    "    shuffled_idx = np.arange(len(y)) # 1,2,...,n\n",
    "    np.random.shuffle(shuffled_idx)\n",
    "    \n",
    "    \n",
    "    for i in range(0, len(y), batch_size):\n",
    "        # Batch indexes\n",
    "        batch_idx = shuffled_idx[i:i+batch_size]\n",
    "        yield X[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: (?, 32, 32, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erobbian\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create new graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholders for the inputs\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    print('Input:', X.shape)\n",
    "    \n",
    "    # Convolutional layer (64 filters, 5x5, stride: 2)\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        X, 64, (5, 5), (2, 2), 'SAME', # \"same\" padding\n",
    "        activation=tf.nn.relu, # ReLU\n",
    "        kernel_initializer=tf.truncated_normal_initializer(stddev=0.01, seed=0),\n",
    "        name='conv1'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Maxpool layer (2x2, stride: 2, \"same\" padding)\n",
    "    pool1 = tf.layers.max_pooling2d(conv1, (2, 2), (2, 2), 'SAME')\n",
    "    \n",
    "    \n",
    "    # Convolutional layer (64 filters, 3x3, stride: 1)\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        pool1, 64, (3, 3), (1, 1), 'SAME', # \"same\" padding\n",
    "        activation=tf.nn.relu, # ReLU\n",
    "        kernel_initializer=tf.truncated_normal_initializer(stddev=0.01, seed=0),\n",
    "        name='conv2'\n",
    "    )\n",
    "   \n",
    "    \n",
    "    # Maxpool layer (2x2, stride: 2, \"same\" padding)\n",
    "    pool2 = tf.layers.max_pooling2d(conv2, (2, 2), (2, 2), 'SAME')\n",
    "    \n",
    "    \n",
    "    # Flatten output\n",
    "    flat_output = tf.contrib.layers.flatten(pool2)\n",
    "  \n",
    "    \n",
    "    # Dropout\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    flat_output = tf.layers.dropout(flat_output, rate=0.5, seed=0, training=training)\n",
    "    \n",
    "    \n",
    "    # Fully connected layer\n",
    "    fc1 = tf.layers.dense(\n",
    "        flat_output, 256, # 256 hidden units\n",
    "        activation=tf.nn.relu, # ReLU\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "        bias_initializer=tf.zeros_initializer()\n",
    "    )\n",
    "   \n",
    "    \n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        fc1, 4, # One output unit per category\n",
    "        activation=None, # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=0),\n",
    "        bias_initializer=tf.zeros_initializer()\n",
    "    )\n",
    "   \n",
    "    \n",
    "    # Kernel of the 1st conv. layer\n",
    "    with tf.variable_scope('conv1', reuse=True):\n",
    "        conv_kernels = tf.get_variable('kernel')\n",
    "    \n",
    "    # Mean cross-entropy\n",
    "    mean_ce = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=y, logits=logits))\n",
    "    \n",
    "    # Adam optimizer\n",
    "    lr = tf.placeholder(dtype=tf.float32)\n",
    "    gd = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "    # Minimize cross-entropy\n",
    "    train_op = gd.minimize(mean_ce)\n",
    "    \n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - valid: 0.252 train: 0.263 (mean)\n",
      "Epoch 2 - valid: 0.424 train: 0.341 (mean)\n",
      "Epoch 3 - valid: 0.488 train: 0.412 (mean)\n",
      "Epoch 4 - valid: 0.463 train: 0.442 (mean)\n",
      "Epoch 5 - valid: 0.505 train: 0.455 (mean)\n",
      "Epoch 6 - valid: 0.504 train: 0.489 (mean)\n",
      "Epoch 7 - valid: 0.507 train: 0.499 (mean)\n",
      "Epoch 8 - valid: 0.506 train: 0.497 (mean)\n",
      "Epoch 9 - valid: 0.510 train: 0.517 (mean)\n",
      "Epoch 10 - valid: 0.514 train: 0.517 (mean)\n",
      "Epoch 11 - valid: 0.533 train: 0.538 (mean)\n",
      "Epoch 12 - valid: 0.482 train: 0.538 (mean)\n",
      "Epoch 13 - valid: 0.544 train: 0.535 (mean)\n",
      "Epoch 14 - valid: 0.553 train: 0.561 (mean)\n",
      "Epoch 15 - valid: 0.522 train: 0.572 (mean)\n",
      "Epoch 16 - valid: 0.543 train: 0.560 (mean)\n",
      "Epoch 17 - valid: 0.564 train: 0.570 (mean)\n",
      "Epoch 18 - valid: 0.555 train: 0.576 (mean)\n",
      "Epoch 19 - valid: 0.572 train: 0.589 (mean)\n",
      "Epoch 20 - valid: 0.591 train: 0.599 (mean)\n",
      "Epoch 21 - valid: 0.582 train: 0.602 (mean)\n",
      "Epoch 22 - valid: 0.598 train: 0.600 (mean)\n",
      "Epoch 23 - valid: 0.589 train: 0.615 (mean)\n",
      "Epoch 24 - valid: 0.586 train: 0.617 (mean)\n",
      "Epoch 25 - valid: 0.596 train: 0.636 (mean)\n",
      "Epoch 26 - valid: 0.609 train: 0.628 (mean)\n",
      "Epoch 27 - valid: 0.610 train: 0.633 (mean)\n",
      "Epoch 28 - valid: 0.627 train: 0.648 (mean)\n",
      "Epoch 29 - valid: 0.601 train: 0.646 (mean)\n",
      "Epoch 30 - valid: 0.614 train: 0.647 (mean)\n",
      "Epoch 31 - valid: 0.629 train: 0.646 (mean)\n",
      "Epoch 32 - valid: 0.619 train: 0.665 (mean)\n",
      "Epoch 33 - valid: 0.629 train: 0.657 (mean)\n",
      "Epoch 34 - valid: 0.644 train: 0.670 (mean)\n",
      "Epoch 35 - valid: 0.636 train: 0.677 (mean)\n",
      "Epoch 36 - valid: 0.627 train: 0.686 (mean)\n",
      "Epoch 37 - valid: 0.640 train: 0.688 (mean)\n",
      "Epoch 38 - valid: 0.636 train: 0.695 (mean)\n",
      "Epoch 39 - valid: 0.625 train: 0.702 (mean)\n",
      "Epoch 40 - valid: 0.636 train: 0.697 (mean)\n",
      "Epoch 41 - valid: 0.641 train: 0.705 (mean)\n",
      "Epoch 42 - valid: 0.649 train: 0.708 (mean)\n",
      "Epoch 43 - valid: 0.616 train: 0.717 (mean)\n",
      "Epoch 44 - valid: 0.646 train: 0.718 (mean)\n",
      "Epoch 45 - valid: 0.652 train: 0.723 (mean)\n",
      "Epoch 46 - valid: 0.651 train: 0.728 (mean)\n",
      "Epoch 47 - valid: 0.655 train: 0.741 (mean)\n",
      "Epoch 48 - valid: 0.656 train: 0.738 (mean)\n",
      "Epoch 49 - valid: 0.651 train: 0.744 (mean)\n",
      "Epoch 50 - valid: 0.650 train: 0.746 (mean)\n",
      "Test accuracy: 0.658\n"
     ]
    }
   ],
   "source": [
    "# Validation accuracy\n",
    "valid_acc_values = []\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Set seed\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # Train several epochs\n",
    "    for epoch in range(50):\n",
    "        # Accuracy values (train) after each batch\n",
    "        batch_acc = []\n",
    "        \n",
    "        for X_batch, y_batch in get_batches(X_train, y_train, 20):\n",
    "            # Run training and evaluate accuracy\n",
    "            _, acc_value = sess.run([train_op, accuracy], feed_dict={\n",
    "                X: X_batch,\n",
    "                y: y_batch,\n",
    "                lr: 0.0001, # Learning rate\n",
    "                training:True\n",
    "            })\n",
    "            \n",
    "            # Save accuracy (current batch)\n",
    "            batch_acc.append(acc_value)\n",
    "\n",
    "        # Evaluate validation accuracy\n",
    "        valid_acc = sess.run(accuracy, feed_dict={\n",
    "            X: X_valid,\n",
    "            y: y_valid,\n",
    "            training: False\n",
    "        })\n",
    "        valid_acc_values.append(valid_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        print('Epoch {} - valid: {:.3f} train: {:.3f} (mean)'.format(\n",
    "            epoch+1, valid_acc, np.mean(batch_acc)\n",
    "        ))\n",
    "        \n",
    "    # Get 1st conv. layer kernels\n",
    "    kernels = conv_kernels.eval()\n",
    "    \n",
    "    # Evaluate test accuracy\n",
    "    test_acc = sess.run(accuracy, feed_dict={\n",
    "        X: X_test,\n",
    "        y: y_test,\n",
    "            training: False\n",
    "    })\n",
    "    print('Test accuracy: {:.3f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
